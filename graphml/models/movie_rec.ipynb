{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arango import ArangoClient\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import requests\n",
    "import sys\n",
    "# import oasis\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from arango import ArangoClient\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch_geometric.data import HeteroData\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = '/Users/ikram.ali/workplace/projects/experiments/downloads/movielens/movies_metadata.csv'\n",
    "df = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on these rows metadata information is missing\n",
    "df = df.drop([19730, 29503, 35587])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled from links.csv file\n",
    "links_small = pd.read_csv('/Users/ikram.ali/workplace/projects/experiments/downloads/movielens/links_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting tmdbId coloumn from links_small file\n",
    "links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['id'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_md = df[df['id'].isin(links_small)]\n",
    "sampled_md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_md['tagline'] = sampled_md['tagline'].fillna('')\n",
    "sampled_md['description'] = sampled_md['overview'] + sampled_md['tagline']\n",
    "sampled_md['description'] = sampled_md['description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_md = sampled_md.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(sampled_md.index, index=sampled_md['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_gen = pd.Series(sampled_md.index, index=sampled_md['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's Load Ratings File\n",
    "\n",
    "We are going to use the ratings file to construct a bipartite graph. This file includes movies rated by different users on the scale of 1-5, rating of 1 implies very bad movie and 5 corresponds to a very good movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_path = '/Users/ikram.ali/workplace/projects/experiments/downloads/movielens/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(ratings_path)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs user and movie mappings\n",
    "def node_mappings(path, index_col):\n",
    "    df = pd.read_csv(path, index_col=index_col)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = node_mappings(ratings_path, index_col='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_mapping = node_mappings(ratings_path, index_col='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_id = ratings_df['movieId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique movie_ids present inside ratings file\n",
    "#m_id = list(set(m_id))\n",
    "m_id = list(dict.fromkeys(m_id))\n",
    "len(m_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = pd.read_csv('/Users/ikram.ali/workplace/projects/experiments/downloads/movielens/links_small.csv')[['movieId', 'tmdbId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map.columns = ['movieId', 'id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = id_map.merge(sampled_md[['title', 'id']], on='id').set_index('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_map = id_map.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 mappings of movieIds\n",
    "list(movie_mapping.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d number of unique movie ids\" %len(m_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ids which dont have meta data information\n",
    "\n",
    "def remove_movies(m_id):\n",
    "    no_metadata = []\n",
    "    for idx in range(len(m_id)):\n",
    "        tmdb_id = id_map.loc[id_map['movieId'] == m_id[idx]]\n",
    "  \n",
    "        if tmdb_id.size == 0:\n",
    "            no_metadata.append(m_id[idx])\n",
    "            #print('No Meta data information at:', m_id[idx])\n",
    "    return no_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_metadata = remove_movies(m_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove ids which dont have meta data information\n",
    "for element in no_metadata:\n",
    "    if element in m_id:\n",
    "        print(\"ids with no metadata information:\",element)\n",
    "        m_id.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of movies with metadata information:\", len(m_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new movie_mapping dict with only m_ids having metadata information\n",
    "movie_mappings = {}\n",
    "for idx, m in enumerate(m_id):\n",
    "    movie_mappings[m] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ArangoClient(hosts=\"http://localhost:8529\")\n",
    "sys_db = client.db(\"_system\", username=\"root\", password=\"toor\")\n",
    "sys_db.databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.db(\"deets\", username=\"root\", password=\"toor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.create_collection(name=\"Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "BATCH_SIZE = 128\n",
    "batch_idx = 1\n",
    "index = 0\n",
    "movie_collection = db[\"Movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading movies metadata information into ArangoDB's Movie collection\n",
    "for idx in tqdm(range(len(m_id))):\n",
    "    insert_doc = {}\n",
    "    tmdb_id = id_map.loc[id_map['movieId'] == m_id[idx]]\n",
    "  \n",
    "    if tmdb_id.size == 0:\n",
    "        print('No Meta data information at:', m_id[idx])\n",
    "        \n",
    "\n",
    "    else:\n",
    "        tmdb_id = int(tmdb_id.iloc[:,1][0])\n",
    "        emb_id = \"Movie/\" + str(movie_mappings[m_id[idx]])\n",
    "        insert_doc[\"_id\"] = emb_id\n",
    "        m_meta = sampled_md.loc[sampled_md['id'] == tmdb_id]\n",
    "        # adding movie metadata information \n",
    "        m_title = m_meta.iloc[0]['title']\n",
    "        m_poster = m_meta.iloc[0]['poster_path']\n",
    "        m_description = m_meta.iloc[0]['description']\n",
    "        m_language = m_meta.iloc[0]['original_language']\n",
    "        m_genre = m_meta.iloc[0]['genres']\n",
    "        m_genre = yaml.load(m_genre, Loader=yaml.BaseLoader)\n",
    "        genres = [g['name'] for g in m_genre]\n",
    "         \n",
    "        insert_doc[\"movieId\"] = m_id[idx]\n",
    "        insert_doc[\"mapped_movieId\"] = movie_mappings[m_id[idx]]\n",
    "        insert_doc[\"tmdbId\"] = tmdb_id\n",
    "        insert_doc['movie_title'] = m_title\n",
    "     \n",
    "        insert_doc['description'] = m_description\n",
    "        insert_doc['genres'] = genres\n",
    "        insert_doc['language'] = m_language\n",
    "        \n",
    "        if str(m_poster) == \"nan\":\n",
    "            insert_doc['poster_path'] = \"No poster path available\"\n",
    "        else:\n",
    "            insert_doc['poster_path'] = m_poster\n",
    "        \n",
    "        batch.append(insert_doc)\n",
    "        index +=1\n",
    "        last_record = (idx == (len(m_id) - 1))\n",
    "        if index % BATCH_SIZE == 0:\n",
    "            #print(\"Inserting batch %d\" % (batch_idx))\n",
    "            batch_idx += 1\n",
    "            movie_collection.import_bulk(batch)\n",
    "            batch = []   \n",
    "        if last_record and len(batch) > 0:\n",
    "            print(\"Inserting batch the last batch!\")\n",
    "            movie_collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new collection named \"Users\" if it does not exist.\n",
    "# This returns an API wrapper for \"Users\" collection.\n",
    "if not db.has_collection(\"Users\"):\n",
    "    db.create_collection(\"Users\", replication_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users has no side information\n",
    "total_users = np.unique(ratings_df[['userId']].values.flatten()).shape[0]\n",
    "print(\"Total number of Users:\", total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_user_collection(total_users):\n",
    "    batch = []\n",
    "    BATCH_SIZE = 50\n",
    "    batch_idx = 1\n",
    "    index = 0\n",
    "    user_ids = list(user_mapping.keys())\n",
    "    user_collection = db[\"Users\"]\n",
    "    for idx in tqdm(range(total_users)):\n",
    "        insert_doc = {}\n",
    "\n",
    "        insert_doc[\"_id\"] = \"Users/\" + str(user_mapping[user_ids[idx]])\n",
    "        insert_doc[\"original_id\"] = str(user_ids[idx])\n",
    "        \n",
    "        batch.append(insert_doc)\n",
    "        index +=1\n",
    "        last_record = (idx == (total_users - 1))\n",
    "        if index % BATCH_SIZE == 0:\n",
    "            #print(\"Inserting batch %d\" % (batch_idx))\n",
    "            batch_idx += 1\n",
    "            user_collection.import_bulk(batch)\n",
    "            batch = []   \n",
    "        if last_record and len(batch) > 0:\n",
    "            print(\"Inserting batch the last batch!\")\n",
    "            user_collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_user_collection(total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Ratings (Edge) Collection\n",
    "\n",
    "Here, we first create a Ratings (Edge) collection in ArangoDB and then populate this collection with edges of a bipartite graph. Each edge document in this collection will contain the information about _from (user) and _to (movie) node along with the rating data given by a user to that particular movie. Once the creation of this collection is completed, a bipartite graph (user and movie nodes) is formed in ArangoDB which can be viewed using ArangoDB Web UI under the Graphs->movie_rating_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new collection named \"Ratings\" if it does not exist.\n",
    "# This returns an API wrapper for \"Ratings\" collection.\n",
    "if not db.has_collection(\"Ratings\"):\n",
    "    db.create_collection(\"Ratings\", edge=True, replication_factor=3)\n",
    "\n",
    "\n",
    "\n",
    "# defining graph schema\n",
    "\n",
    "# create a new graph called movie_rating_graph in the temp database if it does not already exist.\n",
    "if not db.has_graph(\"movie_rating_graph\"):\n",
    "    db.create_graph('movie_rating_graph', smart=True)\n",
    "\n",
    "# This returns and API wrapper for the above created graphs\n",
    "movie_rating_graph = db.graph(\"movie_rating_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vertex collection named \"Users\" if it does not exist.\n",
    "if not movie_rating_graph.has_vertex_collection(\"Users\"):\n",
    "    movie_rating_graph.vertex_collection(\"Users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vertex collection named \"Movie\" if it does not exist.\n",
    "if not movie_rating_graph.has_vertex_collection(\"Movie\"):\n",
    "    movie_rating_graph.vertex_collection(\"Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating edge definitions named \"Ratings. This creates any missing\n",
    "# collections and returns an API wrapper for \"Ratings\" edge collection.\n",
    "if not movie_rating_graph.has_edge_definition(\"Ratings\"):\n",
    "    Ratings = movie_rating_graph.create_edge_definition(\n",
    "        edge_collection='Ratings',\n",
    "        from_vertex_collections=['Users'],\n",
    "        to_vertex_collections=['Movie']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, movie_id, ratings = ratings_df[['userId']].values.flatten(), ratings_df[['movieId']].values.flatten() , ratings_df[['rating']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_graph(user_id, movie_id, ratings):\n",
    "    batch = []\n",
    "    BATCH_SIZE = 100\n",
    "    batch_idx = 1\n",
    "    index = 0\n",
    "    edge_collection = db[\"Ratings\"]\n",
    "    for idx in tqdm(range(ratings.shape[0])):\n",
    "        \n",
    "        # removing edges (movies) with no metatdata\n",
    "        if movie_id[idx] in no_metadata:\n",
    "            print('Removing edges with no metadata', movie_id[idx])\n",
    "            \n",
    "        else:\n",
    "            insert_doc = {}\n",
    "            insert_doc = {\"_id\":    \"Ratings\" + \"/\" + 'user-' + str(user_mapping[user_id[idx]]) + \"-r-\" + \"movie-\" + str(movie_mappings[movie_id[idx]]), \n",
    "                          \"_from\":  (\"Users\" + \"/\" + str(user_mapping[user_id[idx]])),\n",
    "                          \"_to\":    (\"Movie\" + \"/\" + str(movie_mappings[movie_id[idx]])),\n",
    "                          \"_rating\": float(ratings[idx])}\n",
    "\n",
    "            batch.append(insert_doc)\n",
    "            index += 1\n",
    "            last_record = (idx == (ratings.shape[0] - 1))\n",
    "\n",
    "            if index % BATCH_SIZE == 0:\n",
    "                #print(\"Inserting batch %d\" % (batch_idx))\n",
    "                batch_idx += 1\n",
    "                edge_collection.import_bulk(batch)\n",
    "                batch = []\n",
    "            if last_record and len(batch) > 0:\n",
    "                print(\"Inserting batch the last batch!\")\n",
    "                edge_collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ratings_graph(user_id, movie_id, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API wrappers for collections.\n",
    "users = db.collection('Users')\n",
    "movies = db.collection('Movie')\n",
    "ratings_graph = db.collection('Ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users), len(movies), len(ratings_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load edges from Ratings collection in ArangoDB and export them to PyG data format.\n",
    "Data handling of graphs in PyG: In order to construct edges of the graph in PyG we need to represent graph connectivity in COO format (edge_index) i.e with shape [2, num_edges]. Therefore, create_pyg_edges method can be seen as a generic function which reads the documents from edge collection (Ratings) and create edges (edge_index) in PyG using _from (src) and _to (dst) attributes of rating documents. Since the edge of the graph is accompanied with ratings information, hence, create_pyg_edges method is also going to read the _rating attribute from an edge_collection and store it in a PyG data object using edge_attr variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pyg_edges(rating_docs):\n",
    "    src = []\n",
    "    dst = []\n",
    "    ratings = []\n",
    "    for doc in rating_docs:\n",
    "        _from = int(doc['_from'].split('/')[1])\n",
    "        _to   = int(doc['_to'].split('/')[1])\n",
    "         \n",
    "        src.append(_from)\n",
    "        dst.append(_to)\n",
    "        ratings.append(int(doc['_rating']))\n",
    "        \n",
    "    edge_index = torch.tensor([src, dst])\n",
    "    edge_attr = torch.tensor(ratings)\n",
    "\n",
    "    return edge_index, edge_attr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, edge_label = create_pyg_edges(db.aql.execute('FOR doc IN Ratings RETURN doc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_index.shape)\n",
    "print(edge_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load nodes from Ratings collection in ArangoDB and export them PyG data format.\n",
    "So, in the above section we read the \"Ratings” edge collection from ArangoDB and exported edges into PyG acceptable data format i.e edge_index and edge_label. Now, the next step would be to construct movie node features, in order to construct them, I have written the two following methods:\n",
    "\n",
    "Sequence Encoder: This method takes two arguments, the first one is movie_docs with the help of which we can access metadata information of each movie stored inside the \"Movie\" collection. The second argument is model_name which takes a pretrained NLP (based on transformers) model from the SentenceTransformers library and generates text embeddings. In this blogpost, I am generating embeddings for movie titles and representing it as a movie node feature. However, instead of movie title we can also use movie description attribute to generate embeddings for movie nodes. Curious readers can try this out and see if results get better.\n",
    "\n",
    "Genres Encoder: In this method we perform the one-hot-encodings of the genres present inside the Movie collection.\n",
    "\n",
    "Once, the features are generated from sequence encoder and genre encoder method, we concatenate these two feature vectors to construct one feature vector for a movie node.\n",
    "\n",
    "Note: This process of feature generation for movie nodes is inspired from PyG examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequenceEncoder(movie_docs , model_name=None):\n",
    "    movie_titles = [doc['movie_title'] for doc in movie_docs]\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    title_embeddings = model.encode(movie_titles, show_progress_bar=True,\n",
    "                              convert_to_tensor=True, device=device)\n",
    "    \n",
    "    return title_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenresEncoder(movie_docs):\n",
    "    gen = []\n",
    "    #sep = '|'\n",
    "    for doc in movie_docs:\n",
    "        gen.append(doc['genres'])\n",
    "        #genre = doc['movie_genres']\n",
    "        #gen.append(genre.split(sep))\n",
    "    \n",
    "    # getting unique genres\n",
    "    unique_gen = set(list(itertools.chain(*gen)))\n",
    "    print(\"Number of unqiue genres we have:\", unique_gen)\n",
    "    \n",
    "    mapping = {g: i for i, g in enumerate(unique_gen)}\n",
    "    x = torch.zeros(len(gen), len(mapping))\n",
    "    for i, m_gen in enumerate(gen):\n",
    "        for genre in m_gen:\n",
    "            x[i, mapping[genre]] = 1\n",
    "    return x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_emb = SequenceEncoder(db.aql.execute('FOR doc IN Movie RETURN doc'), model_name='all-MiniLM-L6-v2')\n",
    "encoded_genres = GenresEncoder(db.aql.execute('FOR doc IN Movie RETURN doc'))\n",
    "print('Title Embeddings shape:', title_emb.shape)\n",
    "print(\"Encoded Genres shape:\", encoded_genres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat title and genres features of movies\n",
    "movie_x = torch.cat((title_emb, encoded_genres), dim=-1)\n",
    "print(\"Shape of the concatenated features:\", movie_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating PyG Heterogeneous Graph\n",
    "\n",
    "Heterogeneous graphs are those graphs which have different types of nodes and edges in the graph for e.g. Knowledge Graphs. The bipartite graph which we have stored in ArangoDB is also a heterogeneous graph since it constitutes two types of nodes in it i.e. user and movie nodes. Therefore, our next step would be to export the graph present inside ArangoDB to a PyG heterogeneous data object.\n",
    "\n",
    "Since now we have PyG edges, labels and node feature matrix, the next step would be to add these tensors to PyG HeteroData object in order to construct a heterogeneous graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'].num_nodes = len(users)  # Users do not have any features.\n",
    "data['movie'].x = movie_x\n",
    "data['user', 'rates', 'movie'].edge_index = edge_index\n",
    "data['user', 'rates', 'movie'].edge_label = edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['user'].num_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert data into an appropriate format for training a graph-based machine learning model:\n",
    "\n",
    "Here, ToUndirected() transforms a directed graph into (the PyG representation of) an undirected graph, by adding reverse edges for all edges in the graph. Thus, future message passing is performed in both direction of all edges. The function may add reverse edge types to the heterogeneous graph, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing.\n",
    "data = ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
